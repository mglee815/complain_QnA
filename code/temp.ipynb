{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100000 entries, 597127 to 343594\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   제목      100000 non-null  object\n",
      " 1   본문      99979 non-null   object\n",
      " 2   민원발생지   100000 non-null  object\n",
      " 3   접수기관    100000 non-null  object\n",
      " 4   token   100000 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 4.6+ MB\n",
      "None\n",
      "train len : 90000, test len : 10000\n",
      "get bertmodel and vocab\n",
      "using cached model. /home/mglee/VSCODE/git_folder/complain_department_classification/code/.cache/kobert_v1.zip\n",
      "using cached model. /home/mglee/VSCODE/git_folder/complain_department_classification/code/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "data setting\n",
      "using cached model. /home/mglee/VSCODE/git_folder/complain_department_classification/code/.cache/kobert_news_wiki_ko_cased-1087f8699e.spiece\n",
      "Train Start\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2e792f98504a70a060cfd692f9e778",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 / 22500 loss 5.956815719604492 train acc 0.0\n",
      "epoch 1 batch id 101 / 22500 loss 5.856468200683594 train acc 0.0024752475247524753\n",
      "epoch 1 batch id 201 / 22500 loss 5.930868625640869 train acc 0.004975124378109453\n",
      "epoch 1 batch id 301 / 22500 loss 5.782311916351318 train acc 0.01910299003322259\n",
      "epoch 1 batch id 401 / 22500 loss 5.366364479064941 train acc 0.038029925187032416\n",
      "epoch 1 batch id 501 / 22500 loss 5.004795074462891 train acc 0.05538922155688623\n",
      "epoch 1 batch id 601 / 22500 loss 4.809051513671875 train acc 0.06364392678868552\n",
      "epoch 1 batch id 701 / 22500 loss 4.071645736694336 train acc 0.07346647646219687\n",
      "epoch 1 batch id 801 / 22500 loss 4.392438888549805 train acc 0.08114856429463171\n",
      "epoch 1 batch id 901 / 22500 loss 5.835740089416504 train acc 0.08546059933407325\n",
      "epoch 1 batch id 1001 / 22500 loss 4.386695861816406 train acc 0.09090909090909091\n",
      "epoch 1 batch id 1101 / 22500 loss 4.859184265136719 train acc 0.09990917347865577\n",
      "epoch 1 batch id 1201 / 22500 loss 3.7529354095458984 train acc 0.10324729392173189\n",
      "epoch 1 batch id 1301 / 22500 loss 5.165536403656006 train acc 0.11029976940814758\n",
      "epoch 1 batch id 1401 / 22500 loss 2.1782007217407227 train acc 0.11563169164882227\n",
      "epoch 1 batch id 1501 / 22500 loss 5.820052146911621 train acc 0.12491672218520986\n",
      "epoch 1 batch id 1601 / 22500 loss 4.243091583251953 train acc 0.13007495315427858\n",
      "epoch 1 batch id 1701 / 22500 loss 4.885636806488037 train acc 0.13594944150499705\n",
      "epoch 1 batch id 1801 / 22500 loss 4.941229820251465 train acc 0.1390893947806774\n",
      "epoch 1 batch id 1901 / 22500 loss 4.013888835906982 train acc 0.14334560757496054\n",
      "epoch 1 batch id 2001 / 22500 loss 3.4997336864471436 train acc 0.14730134932533734\n",
      "epoch 1 batch id 2101 / 22500 loss 3.3300960063934326 train acc 0.15516420752022847\n",
      "epoch 1 batch id 2201 / 22500 loss 3.4885411262512207 train acc 0.16185824625170378\n",
      "epoch 1 batch id 2301 / 22500 loss 4.025722980499268 train acc 0.16829639287266407\n",
      "epoch 1 batch id 2401 / 22500 loss 4.215059280395508 train acc 0.17221990837151188\n",
      "epoch 1 batch id 2501 / 22500 loss 2.4247145652770996 train acc 0.17872850859656136\n",
      "epoch 1 batch id 2601 / 22500 loss 2.007350444793701 train acc 0.18319876970396\n",
      "epoch 1 batch id 2701 / 22500 loss 2.106388807296753 train acc 0.18659755646057016\n",
      "epoch 1 batch id 2801 / 22500 loss 4.368246078491211 train acc 0.1920742591931453\n",
      "epoch 1 batch id 2901 / 22500 loss 3.4430770874023438 train acc 0.1980351602895553\n",
      "epoch 1 batch id 3001 / 22500 loss 2.3161087036132812 train acc 0.20393202265911362\n",
      "epoch 1 batch id 3101 / 22500 loss 3.4244608879089355 train acc 0.20944856497903902\n",
      "epoch 1 batch id 3201 / 22500 loss 2.176508665084839 train acc 0.21368322399250234\n",
      "epoch 1 batch id 3301 / 22500 loss 3.3422539234161377 train acc 0.21803998788245987\n",
      "epoch 1 batch id 3401 / 22500 loss 2.6317687034606934 train acc 0.22294913260805646\n",
      "epoch 1 batch id 3501 / 22500 loss 3.133345603942871 train acc 0.22879177377892032\n",
      "epoch 1 batch id 3601 / 22500 loss 4.111715793609619 train acc 0.23389336295473478\n",
      "epoch 1 batch id 3701 / 22500 loss 3.0199246406555176 train acc 0.23865171575249933\n",
      "epoch 1 batch id 3801 / 22500 loss 2.051464796066284 train acc 0.2440147329650092\n",
      "epoch 1 batch id 3901 / 22500 loss 1.9057130813598633 train acc 0.24891053576006153\n",
      "epoch 1 batch id 4001 / 22500 loss 3.282017707824707 train acc 0.25362409397650587\n",
      "epoch 1 batch id 4101 / 22500 loss 2.934915065765381 train acc 0.25841258229700076\n",
      "epoch 1 batch id 4201 / 22500 loss 2.6311521530151367 train acc 0.2625565341585337\n",
      "epoch 1 batch id 4301 / 22500 loss 2.441727876663208 train acc 0.2667402929551267\n",
      "epoch 1 batch id 4401 / 22500 loss 2.4092049598693848 train acc 0.2716428084526244\n",
      "epoch 1 batch id 4501 / 22500 loss 4.161779403686523 train acc 0.2770495445456565\n",
      "epoch 1 batch id 4601 / 22500 loss 2.7118332386016846 train acc 0.28216692023473156\n",
      "epoch 1 batch id 4701 / 22500 loss 1.4147058725357056 train acc 0.2869070410550947\n",
      "epoch 1 batch id 4801 / 22500 loss 4.1281256675720215 train acc 0.29004374088731516\n",
      "epoch 1 batch id 4901 / 22500 loss 3.681835651397705 train acc 0.29402162823913486\n",
      "epoch 1 batch id 5001 / 22500 loss 1.7129287719726562 train acc 0.2978904219156169\n",
      "epoch 1 batch id 5101 / 22500 loss 0.819277822971344 train acc 0.30239168790433246\n",
      "epoch 1 batch id 5201 / 22500 loss 3.646334409713745 train acc 0.30609498173428185\n",
      "epoch 1 batch id 5301 / 22500 loss 1.8591251373291016 train acc 0.3102716468590832\n",
      "epoch 1 batch id 5401 / 22500 loss 2.466447591781616 train acc 0.31355304573227183\n",
      "epoch 1 batch id 5501 / 22500 loss 1.9335196018218994 train acc 0.31739683693873844\n",
      "epoch 1 batch id 5601 / 22500 loss 2.4653735160827637 train acc 0.3207909301910373\n",
      "epoch 1 batch id 5701 / 22500 loss 3.359370231628418 train acc 0.3246798807226802\n",
      "epoch 1 batch id 5801 / 22500 loss 3.6326916217803955 train acc 0.3275728322702982\n",
      "epoch 1 batch id 5901 / 22500 loss 4.245956897735596 train acc 0.33100321979325537\n",
      "epoch 1 batch id 6001 / 22500 loss 1.0075856447219849 train acc 0.3345692384602566\n",
      "epoch 1 batch id 6101 / 22500 loss 0.6401882767677307 train acc 0.33838714964759875\n",
      "epoch 1 batch id 6201 / 22500 loss 2.5428977012634277 train acc 0.34139654894371874\n",
      "epoch 1 batch id 6301 / 22500 loss 1.9903532266616821 train acc 0.34450880812569434\n",
      "epoch 1 batch id 6401 / 22500 loss 1.6327979564666748 train acc 0.3477191063896266\n",
      "epoch 1 batch id 6501 / 22500 loss 1.2183233499526978 train acc 0.35075373019535455\n",
      "epoch 1 batch id 6601 / 22500 loss 1.690001130104065 train acc 0.3537721557339797\n",
      "epoch 1 batch id 6701 / 22500 loss 2.3429296016693115 train acc 0.35636472168333083\n",
      "epoch 1 batch id 6801 / 22500 loss 2.5709211826324463 train acc 0.35976327010733716\n",
      "epoch 1 batch id 6901 / 22500 loss 1.6357147693634033 train acc 0.3624836980147805\n",
      "epoch 1 batch id 7001 / 22500 loss 3.5010929107666016 train acc 0.36569775746321953\n",
      "epoch 1 batch id 7101 / 22500 loss 2.0692825317382812 train acc 0.3683988170680186\n",
      "epoch 1 batch id 7201 / 22500 loss 2.3386476039886475 train acc 0.370712401055409\n",
      "epoch 1 batch id 7301 / 22500 loss 2.5234127044677734 train acc 0.37299684974661007\n",
      "epoch 1 batch id 7401 / 22500 loss 0.362135648727417 train acc 0.3754222402378057\n",
      "epoch 1 batch id 7501 / 22500 loss 3.409914016723633 train acc 0.37814958005599253\n",
      "epoch 1 batch id 7601 / 22500 loss 2.872022867202759 train acc 0.3803446914879621\n",
      "epoch 1 batch id 7701 / 22500 loss 1.4247044324874878 train acc 0.3827749642903519\n",
      "epoch 1 batch id 7801 / 22500 loss 1.0891207456588745 train acc 0.38501474169978206\n",
      "epoch 1 batch id 7901 / 22500 loss 1.4612207412719727 train acc 0.3877040880901152\n",
      "epoch 1 batch id 8001 / 22500 loss 1.6846346855163574 train acc 0.3898575178102737\n",
      "epoch 1 batch id 8101 / 22500 loss 3.5694704055786133 train acc 0.3923898284162449\n",
      "epoch 1 batch id 8201 / 22500 loss 0.8883569240570068 train acc 0.3947689306182173\n",
      "epoch 1 batch id 8301 / 22500 loss 2.517904281616211 train acc 0.3973316467895434\n",
      "epoch 1 batch id 8401 / 22500 loss 0.2723292410373688 train acc 0.3992084275681467\n",
      "epoch 1 batch id 8501 / 22500 loss 2.6509926319122314 train acc 0.4016880367015645\n",
      "epoch 1 batch id 8601 / 22500 loss 1.390557050704956 train acc 0.4038774561097547\n",
      "epoch 1 batch id 8701 / 22500 loss 3.2002875804901123 train acc 0.40595908516262497\n",
      "epoch 1 batch id 8801 / 22500 loss 1.2658438682556152 train acc 0.4080502215657312\n",
      "epoch 1 batch id 8901 / 22500 loss 1.6356912851333618 train acc 0.41017863161442536\n",
      "epoch 1 batch id 9001 / 22500 loss 3.8845138549804688 train acc 0.4124819464503944\n",
      "epoch 1 batch id 9101 / 22500 loss 0.578639566898346 train acc 0.4140753763322712\n",
      "epoch 1 batch id 9201 / 22500 loss 2.049511194229126 train acc 0.4157700249972829\n",
      "epoch 1 batch id 9301 / 22500 loss 1.1233289241790771 train acc 0.4178314159767767\n",
      "epoch 1 batch id 9401 / 22500 loss 2.3702099323272705 train acc 0.41998191681735986\n",
      "epoch 1 batch id 9501 / 22500 loss 3.404646396636963 train acc 0.4221134617408694\n",
      "epoch 1 batch id 9601 / 22500 loss 1.4791529178619385 train acc 0.42323716279554213\n",
      "epoch 1 batch id 9701 / 22500 loss 0.3421189785003662 train acc 0.42487887846613753\n",
      "epoch 1 batch id 9801 / 22500 loss 0.785120964050293 train acc 0.42694622997653303\n",
      "epoch 1 batch id 9901 / 22500 loss 2.4578559398651123 train acc 0.428997071002929\n",
      "epoch 1 batch id 10001 / 22500 loss 1.6413124799728394 train acc 0.4307819218078192\n",
      "epoch 1 batch id 10101 / 22500 loss 1.493740200996399 train acc 0.43233343233343235\n",
      "epoch 1 batch id 10201 / 22500 loss 1.474463939666748 train acc 0.43400156847367904\n",
      "epoch 1 batch id 10301 / 22500 loss 4.216343879699707 train acc 0.4357586642073585\n",
      "epoch 1 batch id 10401 / 22500 loss 2.273418426513672 train acc 0.4370252860301894\n",
      "epoch 1 batch id 10501 / 22500 loss 3.0206170082092285 train acc 0.43857727835444243\n",
      "epoch 1 batch id 10601 / 22500 loss 0.5735169649124146 train acc 0.44009999056692767\n",
      "epoch 1 batch id 10701 / 22500 loss 0.20138582587242126 train acc 0.4419914026726474\n",
      "epoch 1 batch id 10801 / 22500 loss 0.19990351796150208 train acc 0.44391722988612164\n",
      "epoch 1 batch id 10901 / 22500 loss 0.3026168942451477 train acc 0.4454637189248693\n",
      "epoch 1 batch id 11001 / 22500 loss 0.06121131032705307 train acc 0.44730024543223346\n",
      "epoch 1 batch id 11101 / 22500 loss 3.2418222427368164 train acc 0.44860823349247814\n",
      "epoch 1 batch id 11201 / 22500 loss 1.36635160446167 train acc 0.4500714221944469\n",
      "epoch 1 batch id 11301 / 22500 loss 1.0333720445632935 train acc 0.45170781346783473\n",
      "epoch 1 batch id 11401 / 22500 loss 0.152267187833786 train acc 0.45338128234365405\n",
      "epoch 1 batch id 11501 / 22500 loss 0.4495355784893036 train acc 0.45474306582036345\n",
      "epoch 1 batch id 11601 / 22500 loss 1.7894316911697388 train acc 0.45612447202827344\n",
      "epoch 1 batch id 11701 / 22500 loss 1.885061502456665 train acc 0.4575890949491496\n",
      "epoch 1 batch id 11801 / 22500 loss 0.212911456823349 train acc 0.4589441572748072\n",
      "epoch 1 batch id 11901 / 22500 loss 3.7888498306274414 train acc 0.46017141416687674\n",
      "epoch 1 batch id 12001 / 22500 loss 1.6319737434387207 train acc 0.4616281976501958\n",
      "epoch 1 batch id 12101 / 22500 loss 2.217682123184204 train acc 0.4628543095611933\n",
      "epoch 1 batch id 12201 / 22500 loss 2.2503585815429688 train acc 0.4641627735431522\n",
      "epoch 1 batch id 12301 / 22500 loss 1.1059677600860596 train acc 0.46536866921388503\n",
      "epoch 1 batch id 12401 / 22500 loss 1.6479929685592651 train acc 0.46699862914281104\n",
      "epoch 1 batch id 12501 / 22500 loss 1.4243278503417969 train acc 0.4681625469962403\n",
      "epoch 1 batch id 12601 / 22500 loss 0.10882452130317688 train acc 0.4697643044202841\n",
      "epoch 1 batch id 12701 / 22500 loss 1.6502307653427124 train acc 0.4712227383670577\n",
      "epoch 1 batch id 12801 / 22500 loss 1.071334958076477 train acc 0.4723849699242247\n",
      "epoch 1 batch id 12901 / 22500 loss 0.7777834534645081 train acc 0.4733935353848539\n",
      "epoch 1 batch id 13001 / 22500 loss 1.8358327150344849 train acc 0.47498269363895085\n",
      "epoch 1 batch id 13101 / 22500 loss 2.7073557376861572 train acc 0.4764712617357454\n",
      "epoch 1 batch id 13201 / 22500 loss 3.8473875522613525 train acc 0.4776721460495417\n",
      "epoch 1 batch id 13301 / 22500 loss 2.100391387939453 train acc 0.4788737688895572\n",
      "epoch 1 batch id 13401 / 22500 loss 2.207225799560547 train acc 0.4799828371017088\n",
      "epoch 1 batch id 13501 / 22500 loss 0.5375314354896545 train acc 0.48163099029701506\n",
      "epoch 1 batch id 13601 / 22500 loss 1.9043508768081665 train acc 0.4831630027203882\n",
      "epoch 1 batch id 13701 / 22500 loss 1.774966835975647 train acc 0.48449018319830667\n",
      "epoch 1 batch id 13801 / 22500 loss 0.06725456565618515 train acc 0.485598869647127\n",
      "epoch 1 batch id 13901 / 22500 loss 1.4241195917129517 train acc 0.48692540105028415\n",
      "epoch 1 batch id 14001 / 22500 loss 0.6942015290260315 train acc 0.48769730733519034\n",
      "epoch 1 batch id 14101 / 22500 loss 4.405957221984863 train acc 0.48860009928373876\n",
      "epoch 1 batch id 14201 / 22500 loss 1.5422272682189941 train acc 0.48957819871840014\n",
      "epoch 1 batch id 14301 / 22500 loss 0.5909493565559387 train acc 0.4908922452975316\n",
      "epoch 1 batch id 14401 / 22500 loss 0.13266773521900177 train acc 0.49173668495243383\n",
      "epoch 1 batch id 14501 / 22500 loss 2.2089736461639404 train acc 0.4930349631059927\n",
      "epoch 1 batch id 14601 / 22500 loss 1.4203745126724243 train acc 0.49402438189165127\n",
      "epoch 1 batch id 14701 / 22500 loss 0.20551256835460663 train acc 0.49532344738453166\n",
      "epoch 1 batch id 14801 / 22500 loss 0.7081611156463623 train acc 0.49626714411188433\n",
      "epoch 1 batch id 14901 / 22500 loss 4.747438907623291 train acc 0.49729883900409366\n",
      "epoch 1 batch id 15001 / 22500 loss 1.2482545375823975 train acc 0.4983167788814079\n",
      "epoch 1 batch id 15101 / 22500 loss 1.3843615055084229 train acc 0.4994867889543739\n",
      "epoch 1 batch id 15201 / 22500 loss 1.2153372764587402 train acc 0.5005098348792842\n",
      "epoch 1 batch id 15301 / 22500 loss 0.1015622541308403 train acc 0.501878962159336\n",
      "epoch 1 batch id 15401 / 22500 loss 3.289980411529541 train acc 0.5028894227647556\n",
      "epoch 1 batch id 15501 / 22500 loss 1.5488996505737305 train acc 0.5039191019934198\n",
      "epoch 1 batch id 15601 / 22500 loss 2.9859490394592285 train acc 0.5048714825972694\n",
      "epoch 1 batch id 15701 / 22500 loss 0.096923828125 train acc 0.5058276542895357\n",
      "epoch 1 batch id 15801 / 22500 loss 1.4506182670593262 train acc 0.5067559015252199\n",
      "epoch 1 batch id 15901 / 22500 loss 1.9238100051879883 train acc 0.5077825294006666\n",
      "epoch 1 batch id 16001 / 22500 loss 4.335803985595703 train acc 0.5086400849946878\n",
      "epoch 1 batch id 16101 / 22500 loss 0.04809068515896797 train acc 0.5095490963294206\n",
      "epoch 1 batch id 16201 / 22500 loss 2.132317066192627 train acc 0.5105549040182705\n",
      "epoch 1 batch id 16301 / 22500 loss 0.03868145868182182 train acc 0.5115330347831422\n",
      "epoch 1 batch id 16401 / 22500 loss 0.24752198159694672 train acc 0.5124687519053717\n",
      "epoch 1 batch id 16501 / 22500 loss 1.5699994564056396 train acc 0.5134385794800315\n",
      "epoch 1 batch id 16601 / 22500 loss 3.1187617778778076 train acc 0.5147882657671224\n",
      "epoch 1 batch id 16701 / 22500 loss 0.04060431197285652 train acc 0.5156128375546375\n",
      "epoch 1 batch id 16801 / 22500 loss 1.2931830883026123 train acc 0.5165615141955836\n",
      "epoch 1 batch id 16901 / 22500 loss 1.4660460948944092 train acc 0.5176320927755754\n",
      "epoch 1 batch id 17001 / 22500 loss 1.0239622592926025 train acc 0.5186165519675313\n",
      "epoch 1 batch id 17101 / 22500 loss 1.2165518999099731 train acc 0.5196187357464476\n",
      "epoch 1 batch id 17201 / 22500 loss 0.7027730941772461 train acc 0.520652869019243\n",
      "epoch 1 batch id 17301 / 22500 loss 4.3424882888793945 train acc 0.5215160973354142\n",
      "epoch 1 batch id 17401 / 22500 loss 1.9633071422576904 train acc 0.5224268720188495\n",
      "epoch 1 batch id 17501 / 22500 loss 0.27415114641189575 train acc 0.5234843723215816\n",
      "epoch 1 batch id 17601 / 22500 loss 2.3841700553894043 train acc 0.5243736151355036\n",
      "epoch 1 batch id 17701 / 22500 loss 2.2034082412719727 train acc 0.5253375515507599\n",
      "epoch 1 batch id 17801 / 22500 loss 1.1986353397369385 train acc 0.5261361721251615\n",
      "epoch 1 batch id 17901 / 22500 loss 2.68784499168396 train acc 0.5268420758616837\n",
      "epoch 1 batch id 18001 / 22500 loss 2.2146387100219727 train acc 0.5277206821843231\n",
      "epoch 1 batch id 18101 / 22500 loss 0.4262182414531708 train acc 0.5287553173857797\n",
      "epoch 1 batch id 18201 / 22500 loss 1.519118309020996 train acc 0.5296549640129663\n",
      "epoch 1 batch id 18301 / 22500 loss 1.311800241470337 train acc 0.5304218348724113\n",
      "epoch 1 batch id 18401 / 22500 loss 1.6153819561004639 train acc 0.5311939568501712\n",
      "epoch 1 batch id 18501 / 22500 loss 0.11509758234024048 train acc 0.5318901680990217\n",
      "epoch 1 batch id 18601 / 22500 loss 2.2965011596679688 train acc 0.5327939358099026\n",
      "epoch 1 batch id 18701 / 22500 loss 0.8868929147720337 train acc 0.5338618255708251\n",
      "epoch 1 batch id 18801 / 22500 loss 2.0777206420898438 train acc 0.5343864688048509\n",
      "epoch 1 batch id 18901 / 22500 loss 2.510585308074951 train acc 0.535156870006878\n",
      "epoch 1 batch id 19001 / 22500 loss 4.582214832305908 train acc 0.5356954897110678\n",
      "epoch 1 batch id 19101 / 22500 loss 1.5202410221099854 train acc 0.5364902361132925\n",
      "epoch 1 batch id 19201 / 22500 loss 0.04626726731657982 train acc 0.5373287849591167\n",
      "epoch 1 batch id 19301 / 22500 loss 2.198167562484741 train acc 0.5380420703590487\n",
      "epoch 1 batch id 19401 / 22500 loss 2.3952503204345703 train acc 0.5385804855419823\n",
      "epoch 1 batch id 19501 / 22500 loss 1.4679375886917114 train acc 0.5393441361981437\n",
      "epoch 1 batch id 19601 / 22500 loss 1.410980224609375 train acc 0.5400362226417019\n",
      "epoch 1 batch id 19701 / 22500 loss 0.024480566382408142 train acc 0.5407212831835947\n",
      "epoch 1 batch id 19801 / 22500 loss 1.286604642868042 train acc 0.5415761830210596\n",
      "epoch 1 batch id 19901 / 22500 loss 2.4952011108398438 train acc 0.5423596804180695\n",
      "epoch 1 batch id 20001 / 22500 loss 2.2243378162384033 train acc 0.5433603319834008\n",
      "epoch 1 batch id 20101 / 22500 loss 3.1884403228759766 train acc 0.5439032883936122\n",
      "epoch 1 batch id 20201 / 22500 loss 0.21008510887622833 train acc 0.5446388792634028\n",
      "epoch 1 batch id 20301 / 22500 loss 1.6523888111114502 train acc 0.5456012019112358\n",
      "epoch 1 batch id 20401 / 22500 loss 1.23298978805542 train acc 0.5461619528454488\n",
      "epoch 1 batch id 20501 / 22500 loss 1.2852104902267456 train acc 0.5470586800643871\n",
      "epoch 1 batch id 20601 / 22500 loss 1.1386358737945557 train acc 0.5476190476190477\n",
      "epoch 1 batch id 20701 / 22500 loss 2.4410595893859863 train acc 0.5483068450799479\n",
      "epoch 1 batch id 20801 / 22500 loss 2.320429801940918 train acc 0.548976010768713\n",
      "epoch 1 batch id 20901 / 22500 loss 1.1038280725479126 train acc 0.5497105401655423\n",
      "epoch 1 batch id 21001 / 22500 loss 0.06370630115270615 train acc 0.5504142659873339\n",
      "epoch 1 batch id 21101 / 22500 loss 0.9779805541038513 train acc 0.5511350172977584\n",
      "epoch 1 batch id 21201 / 22500 loss 1.284787893295288 train acc 0.551695674732324\n",
      "epoch 1 batch id 21301 / 22500 loss 1.2934006452560425 train acc 0.5522862776395474\n",
      "epoch 1 batch id 21401 / 22500 loss 1.3565831184387207 train acc 0.5528479977571141\n",
      "epoch 1 batch id 21501 / 22500 loss 1.8726006746292114 train acc 0.5534044928142877\n",
      "epoch 1 batch id 21601 / 22500 loss 2.823051929473877 train acc 0.553978982454516\n",
      "epoch 1 batch id 21701 / 22500 loss 2.179075241088867 train acc 0.5546057785355514\n",
      "epoch 1 batch id 21801 / 22500 loss 0.9123757481575012 train acc 0.5552497591853585\n",
      "epoch 1 batch id 21901 / 22500 loss 1.2289047241210938 train acc 0.5558878590018721\n",
      "epoch 1 batch id 22001 / 22500 loss 0.5510119795799255 train acc 0.5565428844143449\n",
      "epoch 1 batch id 22101 / 22500 loss 0.9742268323898315 train acc 0.5572032939685987\n",
      "epoch 1 batch id 22201 / 22500 loss 1.3869117498397827 train acc 0.557959100941399\n",
      "epoch 1 batch id 22301 / 22500 loss 2.788224220275879 train acc 0.5586408681225057\n",
      "epoch 1 batch id 22401 / 22500 loss 0.09162241220474243 train acc 0.559182625775635\n",
      "epoch 1 train acc 0.5597666666666666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e586928ef7f4ff2a21996de0e3777d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test acc 0.7017\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8402295e1f044f8a34aae9fc01d01cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/22500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 / 22500 loss 0.9183655977249146 train acc 0.75\n",
      "epoch 2 batch id 101 / 22500 loss 1.177146077156067 train acc 0.7054455445544554\n",
      "epoch 2 batch id 201 / 22500 loss 2.0242714881896973 train acc 0.7251243781094527\n",
      "epoch 2 batch id 301 / 22500 loss 0.2234921157360077 train acc 0.717607973421927\n",
      "epoch 2 batch id 401 / 22500 loss 0.35814037919044495 train acc 0.7281795511221946\n",
      "epoch 2 batch id 501 / 22500 loss 0.1813473105430603 train acc 0.7250499001996008\n",
      "epoch 2 batch id 601 / 22500 loss 1.1729357242584229 train acc 0.7200499168053245\n",
      "epoch 2 batch id 701 / 22500 loss 1.8668217658996582 train acc 0.7161198288159771\n",
      "epoch 2 batch id 801 / 22500 loss 1.2432953119277954 train acc 0.7144194756554307\n",
      "epoch 2 batch id 901 / 22500 loss 3.2532544136047363 train acc 0.7089345172031076\n",
      "epoch 2 batch id 1001 / 22500 loss 2.0873913764953613 train acc 0.7107892107892108\n",
      "epoch 2 batch id 1101 / 22500 loss 1.4951894283294678 train acc 0.7086739327883742\n",
      "epoch 2 batch id 1201 / 22500 loss 1.7214677333831787 train acc 0.7035803497085762\n",
      "epoch 2 batch id 1301 / 22500 loss 1.2097606658935547 train acc 0.7042659492697925\n",
      "epoch 2 batch id 1401 / 22500 loss 0.0033575897105038166 train acc 0.7041399000713776\n",
      "epoch 2 batch id 1501 / 22500 loss 0.7743711471557617 train acc 0.7071952031978681\n",
      "epoch 2 batch id 1601 / 22500 loss 1.4195791482925415 train acc 0.7064334790755777\n",
      "epoch 2 batch id 1701 / 22500 loss 0.08071479201316833 train acc 0.7088477366255144\n",
      "epoch 2 batch id 1801 / 22500 loss 1.159808874130249 train acc 0.7079400333148251\n",
      "epoch 2 batch id 1901 / 22500 loss 2.8230342864990234 train acc 0.7069963177275118\n",
      "epoch 2 batch id 2001 / 22500 loss 1.315192461013794 train acc 0.7068965517241379\n",
      "epoch 2 batch id 2101 / 22500 loss 0.7197487950325012 train acc 0.7083531651594479\n",
      "epoch 2 batch id 2201 / 22500 loss 1.4061245918273926 train acc 0.710358927760109\n",
      "epoch 2 batch id 2301 / 22500 loss 0.7862446308135986 train acc 0.7088222511951325\n",
      "epoch 2 batch id 2401 / 22500 loss 0.9426307678222656 train acc 0.7098084131611828\n",
      "epoch 2 batch id 2501 / 22500 loss 1.520400047302246 train acc 0.7102159136345462\n",
      "epoch 2 batch id 2601 / 22500 loss 1.1243199110031128 train acc 0.709919261822376\n",
      "epoch 2 batch id 2701 / 22500 loss 1.96652090549469 train acc 0.7098296927064051\n",
      "epoch 2 batch id 2801 / 22500 loss 2.0343639850616455 train acc 0.7101927882898965\n",
      "epoch 2 batch id 2901 / 22500 loss 1.9067270755767822 train acc 0.7092381937263013\n",
      "epoch 2 batch id 3001 / 22500 loss 0.02049347013235092 train acc 0.7096801066311229\n",
      "epoch 2 batch id 3101 / 22500 loss 0.2961141765117645 train acc 0.7102547565301516\n",
      "epoch 2 batch id 3201 / 22500 loss 0.575585663318634 train acc 0.7095438925335833\n",
      "epoch 2 batch id 3301 / 22500 loss 0.1432369351387024 train acc 0.7092547712814299\n",
      "epoch 2 batch id 3401 / 22500 loss 0.021313484758138657 train acc 0.7086886209938253\n",
      "epoch 2 batch id 3501 / 22500 loss 2.598820209503174 train acc 0.7086546700942588\n",
      "epoch 2 batch id 3601 / 22500 loss 0.18987995386123657 train acc 0.7085531796723132\n",
      "epoch 2 batch id 3701 / 22500 loss 2.155277729034424 train acc 0.708930018913807\n",
      "epoch 2 batch id 3801 / 22500 loss 0.6578689813613892 train acc 0.7091554853985793\n",
      "epoch 2 batch id 3901 / 22500 loss 0.03681304305791855 train acc 0.709433478595232\n",
      "epoch 2 batch id 4001 / 22500 loss 1.6574103832244873 train acc 0.709822544363909\n",
      "epoch 2 batch id 4101 / 22500 loss 0.12268214672803879 train acc 0.7106193611314313\n",
      "epoch 2 batch id 4201 / 22500 loss 0.954434335231781 train acc 0.7100095215424899\n",
      "epoch 2 batch id 4301 / 22500 loss 0.045486077666282654 train acc 0.7102999302487794\n",
      "epoch 2 batch id 4401 / 22500 loss 0.07329316437244415 train acc 0.7106339468302658\n",
      "epoch 2 batch id 4501 / 22500 loss 1.3144789934158325 train acc 0.7112863808042658\n",
      "epoch 2 batch id 4601 / 22500 loss 0.44178158044815063 train acc 0.7122364703325365\n",
      "epoch 2 batch id 4701 / 22500 loss 0.9545362591743469 train acc 0.7128802382471815\n",
      "epoch 2 batch id 4801 / 22500 loss 2.2397172451019287 train acc 0.7124557383878358\n",
      "epoch 2 batch id 4901 / 22500 loss 3.4742419719696045 train acc 0.7119975515200979\n",
      "epoch 2 batch id 5001 / 22500 loss 0.3545732796192169 train acc 0.7111577684463107\n",
      "epoch 2 batch id 5101 / 22500 loss 0.0034429363440722227 train acc 0.7121642815134287\n",
      "epoch 2 batch id 5201 / 22500 loss 2.445096015930176 train acc 0.7126033455104788\n",
      "epoch 2 batch id 5301 / 22500 loss 0.04629305377602577 train acc 0.7125070741369552\n",
      "epoch 2 batch id 5401 / 22500 loss 0.08253904432058334 train acc 0.7123217922606925\n",
      "epoch 2 batch id 5501 / 22500 loss 0.04128900542855263 train acc 0.7125068169423742\n",
      "epoch 2 batch id 5601 / 22500 loss 0.1267210692167282 train acc 0.712149616139975\n",
      "epoch 2 batch id 5701 / 22500 loss 1.5075987577438354 train acc 0.7126819856165585\n",
      "epoch 2 batch id 5801 / 22500 loss 3.169642686843872 train acc 0.7128943285640407\n",
      "epoch 2 batch id 5901 / 22500 loss 4.179631233215332 train acc 0.7130571089645823\n",
      "epoch 2 batch id 6001 / 22500 loss 0.022642647847533226 train acc 0.7135477420429929\n",
      "epoch 2 batch id 6101 / 22500 loss 0.012172495946288109 train acc 0.7143910834289461\n",
      "epoch 2 batch id 6201 / 22500 loss 0.023216722533106804 train acc 0.7142799548459926\n",
      "epoch 2 batch id 6301 / 22500 loss 0.5169945359230042 train acc 0.7151245833994604\n",
      "epoch 2 batch id 6401 / 22500 loss 0.7963621020317078 train acc 0.715513201062334\n",
      "epoch 2 batch id 6501 / 22500 loss 0.357207328081131 train acc 0.7158514074757729\n",
      "epoch 2 batch id 6601 / 22500 loss 1.3662115335464478 train acc 0.7164066050598394\n",
      "epoch 2 batch id 6701 / 22500 loss 2.136002540588379 train acc 0.7164602298164453\n",
      "epoch 2 batch id 6801 / 22500 loss 2.1686172485351562 train acc 0.7175047787090134\n",
      "epoch 2 batch id 6901 / 22500 loss 0.17731784284114838 train acc 0.7176496159976815\n",
      "epoch 2 batch id 7001 / 22500 loss 3.3070414066314697 train acc 0.7183616626196258\n",
      "epoch 2 batch id 7101 / 22500 loss 1.3387266397476196 train acc 0.7186311787072244\n",
      "epoch 2 batch id 7201 / 22500 loss 0.07116637378931046 train acc 0.7188237744757673\n",
      "epoch 2 batch id 7301 / 22500 loss 1.69933021068573 train acc 0.7186686755239008\n",
      "epoch 2 batch id 7401 / 22500 loss 0.3179410696029663 train acc 0.7189906769355493\n",
      "epoch 2 batch id 7501 / 22500 loss 2.484462261199951 train acc 0.7195707239034795\n",
      "epoch 2 batch id 7601 / 22500 loss 1.7668256759643555 train acc 0.7199710564399421\n",
      "epoch 2 batch id 7701 / 22500 loss 1.378603458404541 train acc 0.7205882352941176\n",
      "epoch 2 batch id 7801 / 22500 loss 0.11593791097402573 train acc 0.7206127419561594\n",
      "epoch 2 batch id 7901 / 22500 loss 0.9001675248146057 train acc 0.7212694595620808\n",
      "epoch 2 batch id 8001 / 22500 loss 0.6466636657714844 train acc 0.721472315960505\n",
      "epoch 2 batch id 8101 / 22500 loss 3.421412467956543 train acc 0.7216084434020491\n",
      "epoch 2 batch id 8201 / 22500 loss 0.031025927513837814 train acc 0.7219851237653945\n",
      "epoch 2 batch id 8301 / 22500 loss 1.2512739896774292 train acc 0.7228345982411758\n",
      "epoch 2 batch id 8401 / 22500 loss 0.9192560911178589 train acc 0.7230984406618259\n",
      "epoch 2 batch id 8501 / 22500 loss 2.34747314453125 train acc 0.7235619338901306\n",
      "epoch 2 batch id 8601 / 22500 loss 0.47872602939605713 train acc 0.723491454482037\n",
      "epoch 2 batch id 8701 / 22500 loss 0.6786209940910339 train acc 0.7241121710148258\n",
      "epoch 2 batch id 8801 / 22500 loss 0.8189024329185486 train acc 0.7243779116009544\n",
      "epoch 2 batch id 8901 / 22500 loss 1.012377142906189 train acc 0.7247500280867318\n",
      "epoch 2 batch id 9001 / 22500 loss 3.7331132888793945 train acc 0.7253082990778802\n",
      "epoch 2 batch id 9101 / 22500 loss 0.04194818064570427 train acc 0.7253598505658719\n",
      "epoch 2 batch id 9201 / 22500 loss 1.1812654733657837 train acc 0.7254102814911423\n",
      "epoch 2 batch id 9301 / 22500 loss 0.2675035893917084 train acc 0.7254596279969896\n",
      "epoch 2 batch id 9401 / 22500 loss 0.19791190326213837 train acc 0.7255345176045102\n",
      "epoch 2 batch id 9501 / 22500 loss 1.6685497760772705 train acc 0.725923586990843\n",
      "epoch 2 batch id 9601 / 22500 loss 0.15298452973365784 train acc 0.7256275387980419\n",
      "epoch 2 batch id 9701 / 22500 loss 0.031046409159898758 train acc 0.725801463766622\n",
      "epoch 2 batch id 9801 / 22500 loss 0.6619703769683838 train acc 0.7262779308233853\n",
      "epoch 2 batch id 9901 / 22500 loss 2.072381019592285 train acc 0.7267700232299767\n",
      "epoch 2 batch id 10001 / 22500 loss 0.75248122215271 train acc 0.7271272872712728\n",
      "epoch 2 batch id 10101 / 22500 loss 1.1871546506881714 train acc 0.7273784773784774\n",
      "epoch 2 batch id 10201 / 22500 loss 1.145267128944397 train acc 0.7273796686599353\n",
      "epoch 2 batch id 10301 / 22500 loss 1.8420583009719849 train acc 0.7277206096495485\n",
      "epoch 2 batch id 10401 / 22500 loss 1.3474831581115723 train acc 0.7277184886068647\n",
      "epoch 2 batch id 10501 / 22500 loss 3.589005470275879 train acc 0.7278116369869536\n",
      "epoch 2 batch id 10601 / 22500 loss 0.002546097617596388 train acc 0.7278322799735873\n",
      "epoch 2 batch id 10701 / 22500 loss 0.003994204569607973 train acc 0.7285300439211289\n",
      "epoch 2 batch id 10801 / 22500 loss 0.12297796458005905 train acc 0.7290528654754189\n",
      "epoch 2 batch id 10901 / 22500 loss 0.011673955246806145 train acc 0.7292220897165398\n",
      "epoch 2 batch id 11001 / 22500 loss 0.015577307902276516 train acc 0.7295700390873557\n",
      "epoch 2 batch id 11101 / 22500 loss 2.6963860988616943 train acc 0.7298441581839474\n",
      "epoch 2 batch id 11201 / 22500 loss 0.28063154220581055 train acc 0.7301580216052138\n",
      "epoch 2 batch id 11301 / 22500 loss 0.11944223195314407 train acc 0.7303335987965667\n",
      "epoch 2 batch id 11401 / 22500 loss 0.03985629603266716 train acc 0.7304183843522498\n",
      "epoch 2 batch id 11501 / 22500 loss 0.005625342018902302 train acc 0.7305016955047388\n",
      "epoch 2 batch id 11601 / 22500 loss 0.3469592034816742 train acc 0.7306266701146453\n",
      "epoch 2 batch id 11701 / 22500 loss 1.817100167274475 train acc 0.7308349713699683\n",
      "epoch 2 batch id 11801 / 22500 loss 0.03794259577989578 train acc 0.7308914498771291\n",
      "epoch 2 batch id 11901 / 22500 loss 2.3158507347106934 train acc 0.7311150323502227\n",
      "epoch 2 batch id 12001 / 22500 loss 1.6167802810668945 train acc 0.7313348887592701\n",
      "epoch 2 batch id 12101 / 22500 loss 1.7869952917099 train acc 0.7314684736798611\n",
      "epoch 2 batch id 12201 / 22500 loss 0.9083722829818726 train acc 0.7317023194820097\n",
      "epoch 2 batch id 12301 / 22500 loss 0.7992194890975952 train acc 0.7317088041622632\n",
      "epoch 2 batch id 12401 / 22500 loss 0.4624529480934143 train acc 0.7319974195629385\n",
      "epoch 2 batch id 12501 / 22500 loss 0.21583443880081177 train acc 0.7320014398848093\n",
      "epoch 2 batch id 12601 / 22500 loss 0.07799262553453445 train acc 0.732402190302357\n",
      "epoch 2 batch id 12701 / 22500 loss 1.9840620756149292 train acc 0.7327178962286434\n",
      "epoch 2 batch id 12801 / 22500 loss 0.42164579033851624 train acc 0.7328138426685415\n",
      "epoch 2 batch id 12901 / 22500 loss 0.23104128241539001 train acc 0.7329083016820401\n",
      "epoch 2 batch id 13001 / 22500 loss 1.737619161605835 train acc 0.733135912622106\n",
      "epoch 2 batch id 13101 / 22500 loss 2.5491976737976074 train acc 0.7333982138768033\n",
      "epoch 2 batch id 13201 / 22500 loss 1.8667876720428467 train acc 0.7335618513748958\n",
      "epoch 2 batch id 13301 / 22500 loss 1.6662853956222534 train acc 0.7338358018194121\n",
      "epoch 2 batch id 13401 / 22500 loss 1.0108757019042969 train acc 0.734068353107977\n",
      "epoch 2 batch id 13501 / 22500 loss 0.020962689071893692 train acc 0.7343715280349604\n",
      "epoch 2 batch id 13601 / 22500 loss 0.9463512897491455 train acc 0.7347437688405264\n",
      "epoch 2 batch id 13701 / 22500 loss 1.3023947477340698 train acc 0.7349828479673016\n",
      "epoch 2 batch id 13801 / 22500 loss 0.3623293936252594 train acc 0.7349648576190131\n",
      "epoch 2 batch id 13901 / 22500 loss 0.014011052437126637 train acc 0.7353787497302352\n",
      "epoch 2 batch id 14001 / 22500 loss 0.015622392296791077 train acc 0.7353760445682451\n",
      "epoch 2 batch id 14101 / 22500 loss 3.9612135887145996 train acc 0.7352492731011985\n",
      "epoch 2 batch id 14201 / 22500 loss 1.4224860668182373 train acc 0.7353883529328921\n",
      "epoch 2 batch id 14301 / 22500 loss 0.12636320292949677 train acc 0.7357003006782743\n",
      "epoch 2 batch id 14401 / 22500 loss 0.1620601862668991 train acc 0.7355565585723214\n",
      "epoch 2 batch id 14501 / 22500 loss 1.1288864612579346 train acc 0.7359492448796635\n",
      "epoch 2 batch id 14601 / 22500 loss 1.157382845878601 train acc 0.7358913773029244\n",
      "epoch 2 batch id 14701 / 22500 loss 0.2393585741519928 train acc 0.736089381674716\n",
      "epoch 2 batch id 14801 / 22500 loss 0.6795817017555237 train acc 0.7362678197419094\n",
      "epoch 2 batch id 14901 / 22500 loss 5.454955577850342 train acc 0.7363767532380378\n",
      "epoch 2 batch id 15001 / 22500 loss 0.09905694425106049 train acc 0.7366008932737818\n",
      "epoch 2 batch id 15101 / 22500 loss 1.52092707157135 train acc 0.7366234024236805\n",
      "epoch 2 batch id 15201 / 22500 loss 1.3230477571487427 train acc 0.7367936319978948\n",
      "epoch 2 batch id 15301 / 22500 loss 0.013175174593925476 train acc 0.7372393961179008\n",
      "epoch 2 batch id 15401 / 22500 loss 2.732990026473999 train acc 0.7374034153626388\n",
      "epoch 2 batch id 15501 / 22500 loss 0.9854187965393066 train acc 0.7375169343913296\n",
      "epoch 2 batch id 15601 / 22500 loss 1.6773736476898193 train acc 0.7375809242997243\n",
      "epoch 2 batch id 15701 / 22500 loss 0.020729446783661842 train acc 0.7377555569708936\n",
      "epoch 2 batch id 15801 / 22500 loss 0.15907832980155945 train acc 0.7379438010252516\n",
      "epoch 2 batch id 15901 / 22500 loss 2.088043212890625 train acc 0.7382869001949562\n",
      "epoch 2 batch id 16001 / 22500 loss 3.59002947807312 train acc 0.7383757265170927\n",
      "epoch 2 batch id 16101 / 22500 loss 0.012568139471113682 train acc 0.7384789764610894\n",
      "epoch 2 batch id 16201 / 22500 loss 1.4563651084899902 train acc 0.7387815566940312\n",
      "epoch 2 batch id 16301 / 22500 loss 0.013936776667833328 train acc 0.738942396172014\n",
      "epoch 2 batch id 16401 / 22500 loss 0.016828112304210663 train acc 0.7391622462044998\n",
      "epoch 2 batch id 16501 / 22500 loss 1.3888895511627197 train acc 0.739318829161869\n",
      "epoch 2 batch id 16601 / 22500 loss 0.2729053497314453 train acc 0.7397295343653997\n",
      "epoch 2 batch id 16701 / 22500 loss 0.009351355023682117 train acc 0.7398658762948327\n",
      "epoch 2 batch id 16801 / 22500 loss 1.3916540145874023 train acc 0.7400154752693292\n",
      "epoch 2 batch id 16901 / 22500 loss 0.9912588000297546 train acc 0.7403851843086208\n",
      "epoch 2 batch id 17001 / 22500 loss 0.07918265461921692 train acc 0.7405299688253633\n",
      "epoch 2 batch id 17101 / 22500 loss 1.0157383680343628 train acc 0.7407461551956026\n",
      "epoch 2 batch id 17201 / 22500 loss 0.07057586312294006 train acc 0.7409452938782629\n",
      "epoch 2 batch id 17301 / 22500 loss 3.4565606117248535 train acc 0.7410698803537368\n",
      "epoch 2 batch id 17401 / 22500 loss 0.8197307586669922 train acc 0.7412217688638584\n",
      "epoch 2 batch id 17501 / 22500 loss 0.13919465243816376 train acc 0.7415004856865322\n",
      "epoch 2 batch id 17601 / 22500 loss 1.839644432067871 train acc 0.7417050167604113\n",
      "epoch 2 batch id 17701 / 22500 loss 1.0415054559707642 train acc 0.7418366194000339\n",
      "epoch 2 batch id 17801 / 22500 loss 0.5561038851737976 train acc 0.742008875905848\n",
      "epoch 2 batch id 17901 / 22500 loss 1.40250563621521 train acc 0.741955756661639\n",
      "epoch 2 batch id 18001 / 22500 loss 1.6919260025024414 train acc 0.7420837731237153\n",
      "epoch 2 batch id 18101 / 22500 loss 0.32005825638771057 train acc 0.7422518092923043\n",
      "epoch 2 batch id 18201 / 22500 loss 0.14123263955116272 train acc 0.742555354101423\n",
      "epoch 2 batch id 18301 / 22500 loss 2.3813443183898926 train acc 0.7426779957379378\n",
      "epoch 2 batch id 18401 / 22500 loss 1.5604479312896729 train acc 0.7428672354763328\n",
      "epoch 2 batch id 18501 / 22500 loss 0.22170916199684143 train acc 0.7429598400086482\n",
      "epoch 2 batch id 18601 / 22500 loss 2.083986759185791 train acc 0.7431724100854793\n",
      "epoch 2 batch id 18701 / 22500 loss 0.6738916039466858 train acc 0.7433827068071226\n",
      "epoch 2 batch id 18801 / 22500 loss 3.112931489944458 train acc 0.7432583373224828\n",
      "epoch 2 batch id 18901 / 22500 loss 1.3060219287872314 train acc 0.7432675519813766\n",
      "epoch 2 batch id 19001 / 22500 loss 3.264321804046631 train acc 0.7431056260196832\n",
      "epoch 2 batch id 19101 / 22500 loss 1.2263896465301514 train acc 0.7432595152086279\n",
      "epoch 2 batch id 19201 / 22500 loss 0.031168542802333832 train acc 0.7433597208478725\n",
      "epoch 2 batch id 19301 / 22500 loss 0.31223466992378235 train acc 0.7434588881405109\n",
      "epoch 2 batch id 19401 / 22500 loss 1.653012990951538 train acc 0.7434152878717591\n",
      "epoch 2 batch id 19501 / 22500 loss 0.2505972683429718 train acc 0.7435131531716322\n",
      "epoch 2 batch id 19601 / 22500 loss 0.15629911422729492 train acc 0.743507984286516\n",
      "epoch 2 batch id 19701 / 22500 loss 0.0031916245352476835 train acc 0.7435409370082737\n",
      "epoch 2 batch id 19801 / 22500 loss 0.4095621407032013 train acc 0.7437755668905611\n",
      "epoch 2 batch id 19901 / 22500 loss 2.511476516723633 train acc 0.7438822169740215\n",
      "epoch 2 batch id 20001 / 22500 loss 0.7714632749557495 train acc 0.7441502924853758\n",
      "epoch 2 batch id 20101 / 22500 loss 1.5372657775878906 train acc 0.7441172080990995\n",
      "epoch 2 batch id 20201 / 22500 loss 0.10647352784872055 train acc 0.7442453343893867\n",
      "epoch 2 batch id 20301 / 22500 loss 1.0451865196228027 train acc 0.7443968277424757\n",
      "epoch 2 batch id 20401 / 22500 loss 0.9211685061454773 train acc 0.7444610558305965\n",
      "epoch 2 batch id 20501 / 22500 loss 1.10019052028656 train acc 0.7445368518608848\n",
      "epoch 2 batch id 20601 / 22500 loss 2.0283493995666504 train acc 0.7444784233775059\n",
      "epoch 2 batch id 20701 / 22500 loss 1.5062038898468018 train acc 0.7446983237524757\n",
      "epoch 2 batch id 20801 / 22500 loss 1.082741379737854 train acc 0.7448199605788183\n",
      "epoch 2 batch id 20901 / 22500 loss 0.2817825675010681 train acc 0.7449882780728195\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import mxnet\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#=========================================================#\n",
    "\n",
    "##나중에 argpaser로 변경\n",
    "max_grad_norm = 1\n",
    "log_interval = 100\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 2\n",
    "batch_size = 4\n",
    "max_len = 512\n",
    "learning_rate = 5e-5 \n",
    "\n",
    "#########################\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "\n",
    "data = pd.read_pickle(\"../result/sample5_tokenized.pkl\")\n",
    "##############\n",
    "data = data.sample(frac = 0.1)\n",
    "\n",
    "print(data.info())\n",
    "\n",
    "label_to_int = {}\n",
    "for i, item in enumerate(data['접수기관'].unique()):\n",
    "    label_to_int[item] = i\n",
    "\n",
    "data['접수기관'] = data['접수기관'].apply(lambda x : label_to_int[x])\n",
    "data = data[['token', '접수기관']]\n",
    "\n",
    "\n",
    "\n",
    "dataset_train, dataest_test = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "print(f\"train len : {len(dataset_train)}, test len : {len(dataest_test)}\")\n",
    "\n",
    "\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset,bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([\" \".join(dataset.iloc[i]['token'])]) for i in range(len(dataset))]\n",
    "        self.labels = [np.int32(dataset.iloc[i]['접수기관']) for i in range(len(dataset))]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "\n",
    "\n",
    "\n",
    "print('get bertmodel and vocab')\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "\n",
    "print(\"data setting\")\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)\n",
    "\n",
    "train_data = BERTDataset(dataset_train, tok, max_len, True, False)\n",
    "test_data = BERTDataset(dataest_test, tok, max_len, True, False)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size = batch_size, num_workers = 8)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size = batch_size, num_workers = 8)\n",
    "\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = len(dataset_train['접수기관'].unique()),   ##클래스 수 조정##\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)\n",
    "\n",
    "\n",
    "    #BERT 모델 불러오기\n",
    "model = BERTClassifier(bertmodel,  dr_rate=0.5).to(device)\n",
    "\n",
    "#optimizer와 schedule 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n",
    "\n",
    "#정확도 측정을 위한 함수 정의\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc\n",
    "    \n",
    "train_dataloader\n",
    "\n",
    "\n",
    "print(\"Train Start\")\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} / {} loss {} train acc {}\".format(e+1, batch_id+1 , len(train_dataloader), loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm(test_dataloader)):\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        test_acc += calc_accuracy(out, label)\n",
    "    print(\"epoch {} test acc {}\".format(e+1, test_acc / (batch_id+1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../result/kobert/' # google 드라이브 연동 해야함. 관련코드는 뺐음\n",
    "torch.save(model, PATH + 'KoBERT_0621.pt')  # 전체 모델 저장\n",
    "torch.save(model.state_dict(), PATH + 'Kobert_0621_state_dict.pt')  # 모델 객체의 state_dict 저장\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "}, PATH + 'all.tar') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('svmglee')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6f34507fa43ba317958b721fa8398d2051b96ef3f3b32ff98429c26ce06f8cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
